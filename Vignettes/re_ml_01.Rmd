---
title: "re_ml_01"
author: "Nils Tinner"
date: "`r Sys.Date()`"
output: html_document
---

```{r,messager = FALSE}
# Package names
packages <- c("lubridate","tidyverse","visdat","yardstick","purrr","reshape2","rsample","recipes","caret")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}else{
  print("All packages sucessfully installed")
}

invisible(lapply(packages, library, character.only = TRUE))

daily_fluxes <- read_csv("../data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") |>  
  
  # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) |> 
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))

```





```{r}
# linear regression model
source("../R/Evaluate_model_function.R")
source("../R/model_daily_fluxes.R")

  temp_KNN <- model_daily_fluxes(daily_fluxes,"lm",0,0.7)
  print(eval_model(mod = temp_KNN$mod, df_train = temp_KNN$daily_fluxes_train, df_test = temp_KNN$daily_fluxes_test, out = "plot")) 


```


```{R}
# KNN
  temp_KNN <- model_daily_fluxes(daily_fluxes,"knn",8,0.7)
  print(eval_model(mod = temp_KNN$mod, df_train = temp_KNN$daily_fluxes_train, df_test = temp_KNN$daily_fluxes_test, out = "plot")) 

```

##Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?
Because regression predicts what connection there are between variables for all variables and so for both datasets behaves similarly because there is a lot of data in the training part that is all fitted with one line and so the regression line will miss also points of the training data, while nearest neighbour calculates a local regression so it fits training data very well (somehow maybe overfitting?) but may not fit new data as well.

##Why does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?
Because knn applies a linear regression locally over k points. Lm applies the regression over all points so it cannot predicto local variations.

##How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?

Lm no variance and per definition no/very small bias.

KNN can have some bias and a balanced variance -> giving nicer trade-off. If neighbors not to few or too many.


##Visualise temporal variations of observed and modelled GPP for both models, covering all available dates:
```{R}
# KNN
  temp_KNN <- model_daily_fluxes(daily_fluxes,"knn",8,0.7)
  print(eval_model(mod = temp_KNN$mod, df_train = temp_KNN$daily_fluxes_train, df_test = temp_KNN$daily_fluxes_test, out = "plot_all")) 
  temp_KNN <- model_daily_fluxes(daily_fluxes,"lm",NULL,0.7)
  print(eval_model(mod = temp_KNN$mod, df_train = temp_KNN$daily_fluxes_train, df_test = temp_KNN$daily_fluxes_test, out = "plot_all")) 
```
Not much variation in the residuals over time, residuals were used to show the variations of observed and modeled GPP. If there is slight variation like a local increase of the residuals around January of 2008 or the decrease in 2010, this is visible in all models with all data.

##state a hypothesis for how the R2 and the MAE evaluated on the test and on the training set would change fork approaching 1 and for k approaching N (the number of observations in the data). Explain your hypothesis, referring to the bias-variance trade-off.
If k = 1, training data will be predicted with 0 error and perfect R2, but the evaluation will be very poor with a low R2 and a high MAE.
If k approaches n, the training data will show a not optimal R2 and MAE because it is underfitted, and the test data will also not have a optimal fit since variation is not captured as desired.


```{R}

for (k in c(1,5,10,50,100,200)) {
  temp_KNN <- model_daily_fluxes(daily_fluxes,"knn",k,0.7)
  print(eval_model(mod = temp_KNN$mod, df_train = temp_KNN$daily_fluxes_train, df_test = temp_KNN$daily_fluxes_test, out = "plot")) 
}
```


##Is there an “optimal”  k in terms of model generalisability? Edit your code to determine an optimal k
 .
 Idea: Iterate through all to see what scores best R^2 for test case?
 -> change code needed...
 
 
```{R}
rsq_test <-  NULL
values_k <- c(1,seq(5,to = 100, by = 5))
for (k in values_k) {
  
  temp_KNN <- model_daily_fluxes(daily_fluxes,"knn",k,0.7)
  rsq_test <- c(rsq_test, eval_model(mod = temp_KNN$mod, df_train = temp_KNN$daily_fluxes_train, df_test = temp_KNN$daily_fluxes_test, out = "r.squared")$rsq_test)
  
}

rsq_test <- tibble(k = values_k,rsq_test = rsq_test)


ggplot(data = rsq_test,aes(x = k, y = rsq_test))+
  geom_point()+geom_line()+theme_classic()

rsq_test |>
  filter(rsq_test == max(rsq_test))
```