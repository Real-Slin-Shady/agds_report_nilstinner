---
title: "Report Machine Learning 2"
author: "Nils Tinner"
date: "`r Sys.Date()`"
output: html_document
---
# Introduction
In this Report, we want to investigate how models perform across site for GPP prediction. Thereby two sites (Davos and Laegern) will be compared. 
These sites differ in their climates and species compositions.
With this analysis we want to investigate how a model trained on one site behaves if it predicts another site and how a model trained on all the data performes.
We also want to investigate how site characteristics such as species present and climate might have an impact on the model.

# Setup and Data cleaning
First, we load all data and we perform data cleaning and quality controls.


```{r, message = F}

# Package names
packages <- c("lubridate","tidyverse","visdat","yardstick","purrr","reshape2","rsample","recipes","caret","cowplot","stringr","zoo")

source("../R/load_packages.R")
load_packages(packages)



clean_up <- function(df){#Define function for cleanup since duplicated code is smelly
  df <- df |>
  # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    
                SW_IN_F,  
                VPD_F,   
                TA_F,
                ends_with("_QC")
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) |> 
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F))|> 

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC")) 
    return(df)
} 
daily_fluxes_dav <- read_csv("../data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") |>
clean_up()

daily_fluxes_lae <- read_csv("../data/FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv") |>
clean_up()

miss_dav <- vis_miss(
  daily_fluxes_dav,
  cluster = FALSE, 
  warn_large_data = FALSE
  )

miss_lae <- vis_miss(
  daily_fluxes_lae,
  cluster = FALSE, 
  warn_large_data = FALSE
  )

cowplot::plot_grid(miss_lae, miss_dav,
                   labels = c("Laegern",
                              "Davos"))

set.seed(1982)
rm(miss_lae,miss_dav)
```

The loading process is almost the same as in re_ml_1 except that longwave radiation is not kept since there are just too many missing data values that impact our models in davos. Now we have mostly non missing data for the evaluation.

# Within-Site Predictions and Across-Site Predictions
Now we generate the models and select the models with the optimal k for each site.

```{r}
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(daily_fluxes_lae, prop = 0.8, strata = "VPD_F") 
daily_fluxes_lae_train <- rsample::training(split)
daily_fluxes_lae_test <- rsample::testing(split)

split <- rsample::initial_split(daily_fluxes_dav, prop = 0.8, strata = "VPD_F")
daily_fluxes_dav_train <- rsample::training(split)
daily_fluxes_dav_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
source("../R/Recipe_GPP.R")#get recipie function based on the splitted data.
pp_dav <- recipe_GPP(daily_fluxes_dav_train) 
pp_lae <- recipe_GPP(daily_fluxes_lae_train)


mod_cv_dav <- caret::train(pp_dav, 
                       data = daily_fluxes_dav_train |> drop_na(), 
                       method = "knn",
                       trControl = caret::trainControl(method = "cv", number = 10),
                       tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)),
                       metric = "MAE")

mod_cv_lae <- caret::train(pp_lae, 
                       data = daily_fluxes_lae_train |> drop_na(), 
                       method = "knn",
                       trControl = caret::trainControl(method = "cv", number = 10),
                       tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)),
                       metric = "MAE")

print(paste("Optimal k of Davos:",mod_cv_dav$bestTune$k))
print(paste("Optimal k of Laegern:",mod_cv_lae$bestTune$k))

```
We can see that the best k was chosen to be 40 for Davos and 30 for Laegern.


## Data visualisation and across site comparisons
Now we perform a across site evaluation for each model specificly traind on one site.

```{r}
source("../R/Evaluate_model_function.R")
#The evaluate function is quite flexible. Also the names of the plots and the return as plots instead of a print can be specified.
plots_1 <- eval_model(mod_cv_dav,daily_fluxes_dav_train,daily_fluxes_dav_test,plot_name1 = "Training set Davos",plot_name2 = "Test set Davos",out="return_plots")
plots_2 <- eval_model(mod_cv_dav,daily_fluxes_dav_test,daily_fluxes_lae_test, plot_name1 = "Test set Davos",plot_name2 = "Test set Laegern",out="return_plots")


plots_3 <- eval_model(mod_cv_lae,daily_fluxes_lae_train,daily_fluxes_lae_test,plot_name1 = "Training set Laegern",plot_name2 = "Test set Laegern",out="return_plots")
plots_4 <- eval_model(mod_cv_lae,daily_fluxes_lae_test,daily_fluxes_dav_test, plot_name1 = "Test set Laegern",plot_name2 = "Test set Davos",out="return_plots")


title_top <- ggdraw() + 
  draw_label(
    "Trained with Davos data",
    fontface = 'bold',
    x = 0,
    hjust = 0)+
  theme(plot.margin = margin(0, 0, 0, 7)) #Title 1

top_row <- cowplot::plot_grid(plots_1[[1]], plots_1[[2]],plots_2[[2]],
                   nrow = 1) #first row of plots

title_bottom <- ggdraw() + 
  draw_label(
    "Trained with Laegern Data",
    fontface = 'bold',
    x = 0,
    hjust = 0)+
  theme(plot.margin = margin(0, 0, 0, 7)) #Title 2

bootom_row <- cowplot::plot_grid(plots_3[[1]], plots_3[[2]],plots_4[[2]],
                   nrow = 1) #second wor of plots.

plot_grid(
  title_top, top_row,title_bottom,bootom_row,
  ncol = 1,
  rel_heights = c(0.1, 1)) #finally we have the combined plot

rm(title_top,title_bottom,top_row,bootom_row,plots_1,plots_2,plots_3,plots_4) #Cleanup after this mess of a plot

```

## Interpreation
The models both perform much better on the test for the data of the site that they were trained for. This is roughly expected since the situation in site may vary a lot in example tree species and climate. This will be discussed later in more dept.
Overall the model of Davos seems more robust but doesnt perdict the data of Laegern as well as the model of Laegern that is usde to predict the data of Davos.

# Single Model
Now we generate the model with a joined dataset for both sites. 
Then the models are also plotted against each other for discussion purposes.

```{r}
set.seed(1982)  # for reproducibility
daily_fluxes <- bind_rows(daily_fluxes_dav,daily_fluxes_lae,.id = "id")


split <- rsample::initial_split(daily_fluxes, prop = 0.8, strata = "VPD_F")

daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)
source("../R/Recipe_GPP.R")
pp <- recipe_GPP(daily_fluxes_train)
mod_cv <- caret::train(pp, 
                       data = daily_fluxes_train |> drop_na(), 
                       method = "knn",
                       trControl = caret::trainControl(method = "cv", number = 10),
                       tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)),
                       metric = "MAE")

eval_model(mod_cv,daily_fluxes_train,daily_fluxes_test)

daily_fluxes_dav_test <- daily_fluxes_test |>
  filter(id == "1")

#overwriting variables smart -> refactor maybe?
daily_fluxes_lae_test<- daily_fluxes_test |>
  filter(id == "2")

eval_model(mod_cv,daily_fluxes_dav_test,daily_fluxes_lae_test,plot_name1 = "Test Davos",plot_name2 = "Test Laegern",out = "plot_all")



```

Generally, the model performs better than models for single sites used to predict another site.
The model preforms better for davos than laegern. This might be because davos just simply has more datapoints and therefore is weighted heavier than laegern which introduces a bias towards more accurate predictions for davos. This might also be the reason for why this approach is probably not a good idea. It might give more weight for a site that has more datapoints. This could be avoided if both sites had the same number of valid observations. Further reasons for why a single model might be a bad idea will follow.
The bias for Laegern is negative and the bias for Davos is positive.

Lets further explore differences in the predicted values for each site:

```{r}


daily_fluxes_dav_test_plot <- daily_fluxes_dav_test |>
  drop_na()
  daily_fluxes_dav_test_plot$fitted <- predict(mod_cv, newdata =  daily_fluxes_dav_test_plot )
  daily_fluxes_dav_test_plot <- daily_fluxes_dav_test_plot |>
  mutate(yearmonth = format(TIMESTAMP, "%m/%Y"),
         residuals = fitted-GPP_NT_VUT_REF)|>
  group_by(yearmonth)|>
  reframe(residuals = mean(residuals),
            year = year(TIMESTAMP))|>
    mutate(month = str_sub(yearmonth,1,2))


ggplot(data = daily_fluxes_dav_test_plot, aes(x = month, y = residuals, group = year, color = factor(year))) +
  geom_line() +
  labs(title = "Davos",color = "Years")+
  theme_classic()


daily_fluxes_dav_test_plot <- daily_fluxes_dav_test |>
  drop_na()
  daily_fluxes_dav_test_plot$fitted <- predict(mod_cv, newdata =  daily_fluxes_dav_test_plot )
  
daily_fluxes_dav_test_plot |>
    mutate(day = as.Date(format(TIMESTAMP, format="%m-%d"), format = "%m-%d"),
         residuals = fitted-GPP_NT_VUT_REF)|>
  group_by(day)|>
  reframe(residuals = mean(residuals))|>
  ggplot(aes(day, residuals)) +
    geom_point(alpha = 0.3) +
    geom_line(aes(y=rollmean(residuals, 10, na.pad=TRUE))) +
    labs(x = "Date",y = "Residuals", subtitle = "Davos")+ 
    geom_hline(yintercept=0, linetype="dashed", 
               color = "red", linewidth=0.5)+
  scale_x_date(date_breaks = "1 month", date_labels =  "%b") +
    theme_classic()

  
#--------

daily_fluxes_lae_test_plot <- daily_fluxes_lae_test |>
  drop_na()

  daily_fluxes_lae_test_plot$fitted <- predict(mod_cv, newdata =  daily_fluxes_lae_test_plot)
  
  daily_fluxes_lae_test_plot <- daily_fluxes_lae_test_plot |>
  mutate(yearmonth = format(TIMESTAMP, "%m/%Y"),
         residuals = fitted-GPP_NT_VUT_REF)|>
  group_by(yearmonth)|>
  reframe(residuals = mean(residuals),
            year = year(TIMESTAMP))|>
    mutate(month = str_sub(yearmonth,1,2))
  


ggplot(data = daily_fluxes_lae_test_plot, aes(x = month, y = residuals, group = year, color = factor(year))) +
  geom_line() +
  labs(title = "Laegern",color = "Years")+
  theme_classic()



daily_fluxes_lae_test_plot <- daily_fluxes_lae_test |>
  drop_na()
  daily_fluxes_lae_test_plot$fitted <- predict(mod_cv, newdata =  daily_fluxes_lae_test_plot )
  
daily_fluxes_lae_test_plot |>
    mutate(day = as.Date(format(TIMESTAMP, format="%m-%d"), format = "%m-%d"),
         residuals = fitted-GPP_NT_VUT_REF)|>
  group_by(day)|>
  reframe(residuals = mean(residuals))|>
  ggplot(aes(day, residuals)) +
    geom_point(alpha = 0.3) +
    geom_line(aes(y=rollmean(residuals, 10, na.pad=TRUE))) +
    labs(x = "Date",y = "Residuals", subtitle = "Laegern")+ 
    geom_hline(yintercept=0, linetype="dashed", 
               color = "red", linewidth=0.5)+
  scale_x_date(date_breaks = "1 month", date_labels =  "%b") +
    theme_classic()


rm(daily_fluxes_dav_test_plot,daily_fluxes_lae_test_plot)

```

We can now see the deviations for each year as for each month, and then also the mean of all the years as a rolling average.
Overall, again Laegern shows more deviations on average, which might be due to the higher sample size.
The GPP of Laegern is overestimated a bit in the spring, and underestimated a lot in summer.
The GPP of Davos has not a very clear over or underestimation.
These differences might be induced by different climatic conditions facing these two sites, which intern also lead to another species composition.
Laegern is a mixed forest with deciduous trees as well as conifers. Davos is a mountainous site with mostly conifer trees and a much colder climate.
As expected Laegern GPP is underestimated in summer and slightly overestimated in winter/spring. This makes sense since Laegern is a mixed forest with a lot of decidous trees. This means that in winter, no photosynthesis even in favorable conditions can occur. But in summer these species might be more productive than the model expects because the unproductivity might have an influence on these measures. Alternatively, deciduous trees might be simply very productive in the summer to compensate for their inactivity in the winter. This productivity in the summer could be the reason for why we have a negative bias overall in the evaluation dataset of Laegern. To account for the missing of leaves in the winter, data about the leaf coverage could be very useful. 
For Davos the results were a little bit surprising. There, deviations are not as systematic and so the model performes better.
Another source of the inaccuracies of the model could be that the model "mixes" data up and so predicts cross site if the conditions are close. This means that for example a hot summer day in Davos could predict a spring day in Laegern. This could then lead to systematic offsets.
