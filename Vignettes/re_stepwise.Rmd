---
title: "re_stepwise"
author: "Nils Tinner"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: yes
    number_sections : yes
---

# Introduction
This script is an implementation for a stepwise forward regression. The data used is the half hourly fluxes dataset provided by the geocomputation group of the university of Bern and contains a subset of data from the FLUXNET dataset of Davos.
This report not only implements the stepwise forward regression, it also analyses first step results and discusses different implementation possibilities for the stepwise forward regression.
The stepwise forward implementation will be based on the algorithm provided in the script (https://geco-bern.github.io/agds/regressionclassification.html).
The implementation is as follows:

1. Set the number of predictors to be considered to $p=1$.
2. Fit all regression models with $p$ predictors and compute their $R^2$.
3. Select the model with $p$ predictors that achieves the highest $R^2$ (best fitting model) and compute its AIC.
4. Increment to $p+1$. Fit all regression models with $p+1$ predictors that include the predictor selected at the previous step and compute their $R^2$. Select the best fitting model and compute its AIC.
6. If the AIC of the model with $p+1$ predictors is poorer than the AIC of the model with $p$ predictors, retain the model with $p$ predictors and quit. You have found the (presumably) optimal model. Otherwise, continue with with step 4.


# Setup and Loading
At first, we install and load the packages, then we load the dataset halfhourly fluxes. 
At last, we visualize missing data. Multicolinearity is not checked for.

```{r, message=FALSE}
# Package names
packages <- c("lubridate","tidyverse","visdat","yardstick","purrr","reshape2","ggpubr")

source("../R/load_packages.R")
load_packages(packages)

hh_fluxes<- read_csv("../data/halfhourly_data.csv") |>
  select(-ends_with("MDS"),CO2_F_MDS) #Interpolated data is removed except CO2, there only interpolated data is viable...

vis_miss(
  hh_fluxes,
  cluster = FALSE, 
  warn_large_data = FALSE
  ) #Visualize

```

It appears that most data is missing in PPFD_IN. Overall 3.4% of the data is missing.
Data containing MDS data, which is interpolated data, is removed. Only CO2 is kept, because there is no raw form of the data.

# Evaluation of all Bivariate Models
Now, all bivariate models are created. The main goal is to predict GPP, gross primary production, which will be our "target variable". To visualize different predictors and how they compare, a plot is drawn for each one and the R-Square value is shown. This may take some time to load. The R-Squared is chosen because it is the main criterion for later evaluation of the best model for stepwise forward regression.
Also, all r-square values are printed.

```{r}
source("../R/Bivariate_function.R")
r_squared <- bivariate_regressions(hh_fluxes,"GPP_NT_VUT_REF",c("siteid","TIMESTAMP")) |> #bivariate function is called
  purrr::map(~summary(.)$r.squared)

r_squared <- data.frame(value = unlist(r_squared), variable = names(r_squared)) #needed as vector, not a list...

hh_fluxes|>
  select(-siteid,
         -TIMESTAMP)|>
 melt(id.vars='GPP_NT_VUT_REF') |> #melting for plotting

ggplot() +
  geom_jitter(aes(value,GPP_NT_VUT_REF, colour=variable),size = 0.1, na.rm=TRUE) + 
  geom_smooth(aes(value,GPP_NT_VUT_REF), colour="black", method=lm, se=FALSE, na.rm=TRUE,formula = 'y ~ x') +
  geom_text(data = r_squared, aes(label = paste("R^2 = ", round(value,4))), x = Inf, y = Inf, hjust = 1, vjust = 1, size = 1.5, nudge_x = -2, nudge_y = -2) +
  facet_wrap(~variable, scales="free_x")

```

Visible is the best regression: PPFD_IN with a correlation of 0.452. Therefore PPFD_IN will be used as the first predictor in the algorithm later.

Now comes the implementation of the algorithm itself. It is packed to a function to avoid too many variables being loaded to the global environment and to keep it tidy ;) It also facilitates reusage.
The return of the function is saved as the "models" variable. This is a list of all models of the stepwise forward regression.

#Raw Function for stepwise forward
```{r}

source("../R/Stepwise_forward_function.R")
models <- Stepwiseforward_tidy(hh_fluxes,"GPP_NT_VUT_REF",c("TIMESTAMP","siteid"))# The function predicts the target GPP and take all variables as predictors except TIMESTAMP and siteid since these are deemed as non predicting variables.

```

We see that first the "Photosynthetic photon flux density" is chosen (PPFD_IN). This makes sense since these are the very photons that are used for photosynthesis and thus GPP. As the second predictor, Longwave infrared radiation is chosen, which is a bit suprising since longwave infrared radiation is not expected as a extremely good predictor for photosynthesis activity. This might be because there might be colineartiy between the two variables which was not checked for. I'm not going to discuss every predictor since these are also kind of out of my area of specialisation and would require a in dept analysis for each one to discuss the effect on GPP.

Now interestingly while debugging the code, I found that my code was not actually deleting predictors from the used predictors list. That was an issue on my side (as always) but it got me thinking: lets say we do not manage used and unused predictors. There are basically 2 possibilities: first we run out of predictors. There we can just say that the loop should only loop as many times as there are elements in the array. Another case is that it somehow takes twice the same predictors eventough that should not be possible (the R^2 of a new predictor should always increase that's why it should always take one that is not yet used in the model since it increases the R^2) In that case the AIC will just be the same as before. We can assume that the AIC is never the same for two models since it is a precise number and so only is the same if the models is the same, so it added nothing (or here twice the same) and so we exit the loop. 
This implementation lowers the complexity since we dont have to keep track as much of what we did use and what we didn't.
The second implementation is called Stepwise_forward_function_2.R

```{r}

source("../R/Stepwise_forward_function_2.R")
models_2 <- Stepwiseforward_tidy_2(hh_fluxes,"GPP_NT_VUT_REF",c("TIMESTAMP","siteid"))

```
There is a very interesting pattern that emerges from these two versions of the formula. So because the first implementation only always test the remaining predictors and adds them based on whether the AIC decreased, in the end all predictors are added despite that in the last step the R^2 of the new model somehow decreases. I don't exactly know how that is possible but it does. SO the last model actually is worse in terms of the R^2 compared to the previous one.
For the second model which always tests all possible combinations and then selects the best predictor based on R^2 there obviously is a better choice: just take one of the used predictors so that the R^2 will not decrease because its the same model. This then leads to that the AIC is the same as before and so the loop ends. For that reason USTAR is not selected in the second model. 
In the end the first implementation is technically the correct implementation of the algorithm provided but there is an argument to be made that it makes more sense to take the highest R^2 performing model rather then strictly following this algorithm. For the evaluation the strictly correct function, so the first one, will be used.


For the evaluation and discussion plots will be used:

#Visualisation
```{r}


R_squared <- purrr::map(models,~summary(.)$r.squared) |>
  unlist()|>
  as.data.frame()|>
  `colnames<-`("R")|>
  rownames_to_column(var = "rowname")  #get R^2 values in a bit of an improvised way


ggplot(data = R_squared ,aes(y = fct_rev(rowname), x = R)) +
  geom_col(fill = "grey70") +
  scale_x_continuous() +
  labs(y = "Predictors", x = "R-Squared",title = "Plot 1")+
  theme_classic(base_size = 7)+
  geom_text(aes(label=fct_rev(rowname)),position = position_dodge(width = 1),hjust=1, size = 2.5)+ #place text to be inside bar
  theme(axis.text.y = element_blank()) #so that text acually is inside bar



ACI <- purrr::map(models,~extractAIC(.)[2]) |>
  unlist()|>
  as.data.frame()|>
  `colnames<-`("R")|>
  rownames_to_column(var = "rowname") #get AIC for all models


ggplot(data = ACI ,aes(y = fct_rev(rowname), x = R)) +
  geom_col(fill = "grey70") +
  scale_x_continuous() +
  labs(y = "Predictors", x = "ACI-Value",title = "Plot 2")+
  theme_classic(base_size = 7)+
  geom_text(aes(label=fct_rev(rowname)),position = position_dodge(width = 1),hjust=1, size = 2.5)+ #same as before
  theme(axis.text.y = element_blank())


  ggplot(models[[length(models)]], aes(.fitted, .resid))+
    geom_point()+
    stat_smooth(method="loess", formula = 'y ~ x')+geom_hline(yintercept=0, col="red", linetype="dashed")+
    labs(y = "Residuals", x = "Fitted values", title = "Plot 3")+
    theme_classic() #
  
  ggplot(models[[length(models)]],
         aes(.hat, .resid))+ #plot 
    geom_point(aes(size=.cooksd))+
    stat_smooth(method="loess", formula = 'y ~ x')+
    labs(x = "Leverage", y = "Residuals", title = "Plot 4")+
    scale_size_continuous("Cook's Distance", range=c(1,5))+
    theme_classic()+theme(legend.position="bottom")





  
```

In the first and second plot and also in the output of the function it becomes evident that the R-squared value is increasing only in very small increments after the third predictor is added. Also the ACI only becomes only ever so slightly smaller. Though it takes a lot of variables until the ACI starts to increase. Maybe a tougher criterion than a only ever so slightly decreasing AIC should be chosen (but I don't really have the statistical knowledge to back this claim).

For the last model with all predictors that were chosen, a fitted vs residuals plot (Plot 3) and a residuals vs leverage plot (Plot 4) is shown.
The Residual vs Fitted plot for the resulting multivariate model of the stepwise forward regression indicates that the lower fitted values have a positive residual while great vaues tend to be slightly negative. This is not ideal since the distribution should show no visible deviation from the x= 0 line and just be like a random point could. This is probably because negative values of GPP don't make sense and so the residuals must always be positive.
The Residual vs Leverage Plot shows that no single point crosses the Cook's Distance. Generally the points with more leverage seem to fit nicely into the model.

An interesting observation is when we compare the result multivariate regression to the bivariate one. There the order of the best predictors does not correspond to the order in which the predictors are added to the multivariate model. For example the second highest R-Squared value is 0.43 with the predictor SW_IN_F. This predictor is then only added as 5th predictor to the multivariate model. So the ranking of r-squared values of bivariate models does not predict the order in which the predictors are added to the multivariate model.