---
title: "re_stepwise"
author: "Nils Tinner"
date: "`r Sys.Date()`"
output: html_document
---

#Introduction
This script is an implementation for a stepwise forward regression. The data used is the half hourly fluxes dataset provided by the geocomputation group of the university of Bern.

# Setup and Loading
At first, we install and load the packages, then we load the dataset halfhourly fluxes. 
At last, we visualize missing data. Multicolinearity is not checked for.

```{r, message=FALSE}
# Package names
packages <- c("lubridate","tidyverse","visdat","yardstick","purrr","reshape2","ggpubr")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}else{
  print("All packages sucessfully installed")
}

invisible(lapply(packages, library, character.only = TRUE))

hh_fluxes<- read_csv("../data/halfhourly_data.csv")

vis_miss(
  hh_fluxes,
  cluster = FALSE, 
  warn_large_data = FALSE
  )
```
There appears that most data is missing in LW_IN_F_MDS. Also PPFD_IN has a lot of data missing. Overall 5.7% of the data is missing.

# Evaluation of all Bivariate Models
Now all bivariate models are created. To visualize them and how they compare, a plot is drawn for each one and the R-Square value is shown. This may take a long time to load, unsure why. The R-Squared is chosen because it is the main criterion for later evaluation of the best model for stepwise forward regression.
Also all r-square values are printed.

```{r}



source("../R/Bivariate_function.R")
models_bivariate <- bivariate_regressions(hh_fluxes,"GPP_NT_VUT_REF",c("siteid","TIMESTAMP","TA_F_MDS"))

r_squared <- purrr::map(models_bivariate,~summary(.)$r.squared)
r_squared <- data.frame(value = unlist(r_squared), variable = names(r_squared))

hh_fluxes|>
  select(-siteid,
         -TIMESTAMP,
         -TA_F_MDS)|>
 melt(id.vars='GPP_NT_VUT_REF') |>

ggplot() +
  geom_jitter(aes(value,GPP_NT_VUT_REF, colour=variable),size = 0.1, na.rm=TRUE) + 
  geom_smooth(aes(value,GPP_NT_VUT_REF), colour="black", method=lm, se=FALSE, na.rm=TRUE) +
  geom_text(data = r_squared, aes(label = paste("R^2 = ", round(value,4))), x = Inf, y = Inf, hjust = 1, vjust = 1, size = 1.5, nudge_x = -2, nudge_y = -2) +
  facet_wrap(~variable, scales="free_x")

```

Visible is the best regression: PPFD_IN with a correlation of 0.452. Therefore PPFD_IN will be used as the first predictor in the the algorithm later.
We also removed unnecessary or double variables. For example TA_F_MDS is just a version of TA_F with value fill for missing data, we dont want these duplicates in there...

Now comes the implementation of the Algorithm itself. It is packed to a function to avoid too many variables beeing loaded to the global enviroment and to keep it tidy ;) 
The return of the function is saved as the "models" variable.

#Raw Function for stepwise forward
```{r}

source("../R/Stepwise_forward_function.R")
models <- Stepwiseforward_tidy(hh_fluxes,"GPP_NT_VUT_REF",c("TIMESTAMP","siteid","TA_F_MDS"))
```

For the evaluation and discussion also plots will be used:

#Visualisation
```{r}


R_squared <- purrr::map(models,~summary(.)$r.squared) |>
  unlist()|>
  as.data.frame()|>
  `colnames<-`("R")|>
  rownames_to_column(var = "rowname")


ggplot(data = R_squared ,aes(y = fct_rev(rowname), x = R)) +
  geom_col(fill = "grey70") +
  scale_x_continuous() +
  labs(y = "Predictors", x = "R-Squared")+
  theme_classic(base_size = 7)+
  geom_text(aes(label=fct_rev(rowname)),position = position_dodge(width = 1),hjust=1, size = 3)+
  theme(axis.text.y = element_blank())



ACI <- purrr::map(models,~extractAIC(.)[2]) |>
  unlist()|>
  as.data.frame()|>
  `colnames<-`("R")|>
  rownames_to_column(var = "rowname")


ggplot(data = ACI ,aes(y = fct_rev(rowname), x = R)) +
  geom_col(fill = "grey70") +
  scale_x_continuous() +
  labs(y = "Predictors", x = "ACI-Value")+
  theme_classic(base_size = 7)+
  geom_text(aes(label=fct_rev(rowname)),position = position_dodge(width = 1),hjust=1, size = 3)+
  theme(axis.text.y = element_blank())


  ggplot(models[[length(models)]], aes(.fitted, .resid))+
    geom_point()+
    stat_smooth(method="loess")+geom_hline(yintercept=0, col="red", linetype="dashed")+
    xlab("Fitted values")+ylab("Residuals")+
    ggtitle("Residual vs Fitted Plot")+
    theme_bw()
  
  ggplot(models[[length(models)]],
         aes(.hat, .stdresid))+
    geom_point(aes(size=.cooksd), na.rm=TRUE)+
    stat_smooth(method="loess", na.rm=TRUE)+
    xlab("Leverage")+ylab("Standardized Residuals")+
    ggtitle("Residual vs Leverage Plot")+
    scale_size_continuous("Cook's Distance", range=c(1,5))+
    theme_bw()+theme(legend.position="bottom")





  
```

In the first and second plot and also in the output of the function it becomes evident that the R-squared value is increasing only in very small increments after the third predictor is added. Also the ACI only becomes ever so slightly smaller. Though it takes a lot of variables until the ACI starts to increase. 

The Residual vs Fitted plot for the resulting multivariate model of the stepwise forward regression indicates that the lower fitted valueshave a positive residual while great vaues tend to be slightly negative. This is not ideal since the distribution should show no visible deviation from the x= 0 line and just be like a random point could. This may be because negative values dont make sense and so the residuals must always be positive.
The Residual vs Leverage Plot shows that no single point crosses the Cook's Distance. Generally the points with more leverage seem to fit nicely into the model.

An interesting observation is when we compare the resulti multivariate regression to the bivariate one. There the order of the best predictors does not correspond to the order in which the predictors are added to the multivariate model. For example the second highest R-Squared value is 0.43 with the predictor SW_IN_F. This predictor is then only added as 5th predictor to the multivariate model. So the ranking of r-squared values of bivariate models does not predict the order in which the predictors are added to the multivariate model.